"""
Medical NLP Processor
Handles natural language processing for medical text data
"""

import re
import logging
from typing import List, Dict, Any, Optional
import numpy as np

try:
    from transformers import AutoTokenizer, AutoModel
    import torch
    HAS_TRANSFORMERS = True
except ImportError:
    HAS_TRANSFORMERS = False
    logging.warning("Transformers not available. Using basic NLP processing.")

from textblob import TextBlob
from sklearn.feature_extraction.text import TfidfVectorizer

logger = logging.getLogger(__name__)


class MedicalNLPProcessor:
    """
    Processes medical text data for feature extraction and embedding generation
    """
    
    def __init__(self, model_name: str = "bert-base-uncased"):
        self.model_name = model_name
        
        # Medical terminology and mappings
        self.medical_synonyms = {
            'pain': ['ache', 'hurt', 'sore', 'tender', 'discomfort'],
            'nausea': ['sick', 'queasy', 'upset stomach'],
            'fatigue': ['tired', 'exhausted', 'weak', 'energy loss'],
            'fever': ['hot', 'temperature', 'burning up'],
            'headache': ['head pain', 'migraine'],
            'shortness of breath': ['difficulty breathing', 'breathless', 'winded'],
            'dizziness': ['lightheaded', 'vertigo', 'unsteady']
        }
        
        self.severity_indicators = {
            'mild': ['slight', 'little', 'minor', 'mild'],
            'moderate': ['moderate', 'noticeable', 'significant'],
            'severe': ['severe', 'intense', 'extreme', 'unbearable', 'terrible']
        }
        
        self.duration_patterns = {\n            r'\\b(\\d+)\\s*(day|days)\\b': 'days',\n            r'\\b(\\d+)\\s*(week|weeks)\\b': 'weeks',\n            r'\\b(\\d+)\\s*(month|months)\\b': 'months',\n            r'\\b(\\d+)\\s*(year|years)\\b': 'years',\n            r'\\b(\\d+)\\s*(hour|hours)\\b': 'hours'\n        }\n        \n        # Initialize models\n        self._init_models()\n    \n    def _init_models(self):\n        \"\"\"Initialize NLP models and components\"\"\"\n        # Initialize BERT model if transformers is available\n        if HAS_TRANSFORMERS:\n            try:\n                self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n                self.model = AutoModel.from_pretrained(self.model_name)\n                self.model.eval()  # Set to evaluation mode\n                logger.info(f\"Loaded BERT model: {self.model_name}\")\n            except Exception as e:\n                logger.error(f\"Failed to load BERT model: {e}\")\n                HAS_TRANSFORMERS = False\n        \n        # Initialize TF-IDF vectorizer as fallback\n        self.tfidf = TfidfVectorizer(\n            max_features=1000,\n            stop_words='english',\n            ngram_range=(1, 2)\n        )\n        \n        # Medical vocabulary for TF-IDF\n        self.medical_vocabulary = [\n            'pain', 'nausea', 'fever', 'headache', 'fatigue', 'dizziness',\n            'chest pain', 'abdominal pain', 'shortness breath', 'difficulty breathing',\n            'heart rate', 'blood pressure', 'weight loss', 'weight gain'\n        ]\n    \n    def get_text_embedding(self, text: str) -> List[float]:\n        \"\"\"Generate embedding for text using BERT or TF-IDF\"\"\"\n        if not text or text.strip() == \"\":\n            return [0.0] * 768  # Return zero vector\n        \n        if HAS_TRANSFORMERS:\n            return self._get_bert_embedding(text)\n        else:\n            return self._get_tfidf_embedding(text)\n    \n    def _get_bert_embedding(self, text: str) -> List[float]:\n        \"\"\"Generate BERT embedding for text\"\"\"\n        try:\n            # Tokenize text\n            inputs = self.tokenizer(\n                text,\n                return_tensors=\"pt\",\n                max_length=512,\n                truncation=True,\n                padding=True\n            )\n            \n            # Generate embeddings\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n                # Use [CLS] token embedding\n                embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n                return embedding.tolist()\n        \n        except Exception as e:\n            logger.error(f\"Error generating BERT embedding: {e}\")\n            return [0.0] * 768\n    \n    def _get_tfidf_embedding(self, text: str) -> List[float]:\n        \"\"\"Generate TF-IDF embedding for text\"\"\"\n        try:\n            # Fit and transform if not already fitted\n            if not hasattr(self.tfidf, 'vocabulary_'):\n                # Use medical vocabulary to fit TF-IDF\n                self.tfidf.fit(self.medical_vocabulary + [text])\n            \n            # Transform text\n            tfidf_matrix = self.tfidf.transform([text])\n            embedding = tfidf_matrix.toarray()[0].tolist()\n            \n            # Pad to 768 dimensions to match BERT\n            if len(embedding) < 768:\n                embedding.extend([0.0] * (768 - len(embedding)))\n            else:\n                embedding = embedding[:768]\n            \n            return embedding\n        \n        except Exception as e:\n            logger.error(f\"Error generating TF-IDF embedding: {e}\")\n            return [0.0] * 768\n    \n    def extract_symptoms(self, text: str) -> List[Dict[str, Any]]:\n        \"\"\"Extract symptoms and their characteristics from text\"\"\"\n        symptoms = []\n        text_lower = text.lower()\n        \n        # Common symptom patterns\n        symptom_patterns = [\n            r'\\b(pain|ache|hurt|sore)\\b',\n            r'\\b(nausea|sick|queasy)\\b',\n            r'\\b(fever|temperature|hot)\\b',\n            r'\\b(headache|head pain)\\b',\n            r'\\b(fatigue|tired|exhausted)\\b',\n            r'\\b(dizziness|lightheaded|vertigo)\\b',\n            r'\\b(shortness of breath|difficulty breathing)\\b',\n            r'\\b(cough|coughing)\\b',\n            r'\\b(vomit|vomiting|throw up)\\b'\n        ]\n        \n        for pattern in symptom_patterns:\n            matches = re.finditer(pattern, text_lower)\n            for match in matches:\n                symptom = {\n                    'symptom': match.group(0),\n                    'position': match.start(),\n                    'severity': self._extract_severity(text, match.start()),\n                    'duration': self._extract_duration(text)\n                }\n                symptoms.append(symptom)\n        \n        return symptoms\n    \n    def _extract_severity(self, text: str, position: int) -> Optional[str]:\n        \"\"\"Extract severity indicators near symptom mention\"\"\"\n        # Look in a window around the symptom\n        window_start = max(0, position - 50)\n        window_end = min(len(text), position + 50)\n        window_text = text[window_start:window_end].lower()\n        \n        for severity, indicators in self.severity_indicators.items():\n            for indicator in indicators:\n                if indicator in window_text:\n                    return severity\n        \n        return None\n    \n    def _extract_duration(self, text: str) -> Optional[str]:\n        \"\"\"Extract duration information from text\"\"\"\n        text_lower = text.lower()\n        \n        for pattern, unit in self.duration_patterns.items():\n            match = re.search(pattern, text_lower)\n            if match:\n                number = match.group(1)\n                return f\"{number} {unit}\"\n        \n        return None\n    \n    def analyze_sentiment(self, text: str) -> Dict[str, float]:\n        \"\"\"Analyze sentiment of the text\"\"\"\n        try:\n            blob = TextBlob(text)\n            return {\n                'polarity': blob.sentiment.polarity,  # -1 to 1\n                'subjectivity': blob.sentiment.subjectivity  # 0 to 1\n            }\n        except Exception as e:\n            logger.error(f\"Error analyzing sentiment: {e}\")\n            return {'polarity': 0.0, 'subjectivity': 0.0}\n    \n    def extract_medical_entities(self, text: str) -> Dict[str, List[str]]:\n        \"\"\"Extract medical entities from text\"\"\"\n        entities = {\n            'symptoms': [],\n            'body_parts': [],\n            'medications': [],\n            'conditions': []\n        }\n        \n        text_lower = text.lower()\n        \n        # Simple pattern matching for medical entities\n        symptom_keywords = [\n            'pain', 'ache', 'nausea', 'fever', 'headache', 'fatigue',\n            'dizziness', 'cough', 'vomiting', 'diarrhea', 'constipation'\n        ]\n        \n        body_part_keywords = [\n            'head', 'chest', 'abdomen', 'stomach', 'back', 'leg', 'arm',\n            'heart', 'lung', 'liver', 'kidney', 'throat', 'neck'\n        ]\n        \n        condition_keywords = [\n            'diabetes', 'hypertension', 'asthma', 'copd', 'depression',\n            'anxiety', 'arthritis', 'migraine', 'infection'\n        ]\n        \n        # Extract symptoms\n        for keyword in symptom_keywords:\n            if keyword in text_lower:\n                entities['symptoms'].append(keyword)\n        \n        # Extract body parts\n        for keyword in body_part_keywords:\n            if keyword in text_lower:\n                entities['body_parts'].append(keyword)\n        \n        # Extract conditions\n        for keyword in condition_keywords:\n            if keyword in text_lower:\n                entities['conditions'].append(keyword)\n        \n        return entities\n    \n    def normalize_medical_text(self, text: str) -> str:\n        \"\"\"Normalize medical text using synonyms and standard terms\"\"\"\n        normalized_text = text.lower()\n        \n        # Replace synonyms with standard terms\n        for standard_term, synonyms in self.medical_synonyms.items():\n            for synonym in synonyms:\n                normalized_text = normalized_text.replace(synonym, standard_term)\n        \n        return normalized_text\n    \n    def calculate_text_complexity(self, text: str) -> Dict[str, Any]:\n        \"\"\"Calculate various text complexity metrics\"\"\"\n        try:\n            blob = TextBlob(text)\n            sentences = blob.sentences\n            words = blob.words\n            \n            # Basic metrics\n            num_sentences = len(sentences)\n            num_words = len(words)\n            avg_words_per_sentence = num_words / num_sentences if num_sentences > 0 else 0\n            \n            # Character count\n            num_characters = len(text)\n            avg_chars_per_word = num_characters / num_words if num_words > 0 else 0\n            \n            return {\n                'num_sentences': num_sentences,\n                'num_words': num_words,\n                'num_characters': num_characters,\n                'avg_words_per_sentence': avg_words_per_sentence,\n                'avg_chars_per_word': avg_chars_per_word\n            }\n        \n        except Exception as e:\n            logger.error(f\"Error calculating text complexity: {e}\")\n            return {\n                'num_sentences': 0,\n                'num_words': 0,\n                'num_characters': 0,\n                'avg_words_per_sentence': 0,\n                'avg_chars_per_word': 0\n            }
